{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minecraft ReadMe Modeling\n",
    "\n",
    "- [Modeling Begins Here](#Modeling-Begins-Here)\n",
    "- [Module Functions](#Modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import personal modules\n",
    "import prepare as prep\n",
    "#import acquire as ac\n",
    "\n",
    "\n",
    "#import datascience libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# import vizualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Sklearn modules including classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # Gradient Boosting Classifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier # Sklearn version of LGBM Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes Classifier\n",
    "\n",
    "\n",
    "# Sklearn testing, evaluating, and managing model\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# additional, advanced classifiers\n",
    "from xgboost import XGBClassifier as xgb  # XG Boost Classifier\n",
    "from xgboost import DMatrix  # used to transform series to matrix for xg classifier\n",
    "from lightgbm import LGBMClassifier # Light Gradient Boost Classifier\n",
    "from catboost import CatBoostClassifier # Cat boost classifier\n",
    "\n",
    "\n",
    "# import module from standard library\n",
    "from time import time\n",
    "from pprint import pprint # pretty print\n",
    "\n",
    "\n",
    "# NLP related modules / libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk #Natural Language Tool Kit\n",
    "import re   #Regular Expressions\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy():\n",
    "    df = pd.read_csv('clean_scraped_data.csv', index_col=[0])\n",
    "    df = prep.map_other_languages(df)\n",
    "    x = df['lemmatized']\n",
    "    y = df['language']\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    x_vectorized = cv.fit_transform(x)\n",
    "\n",
    "    \n",
    "    \n",
    "    return x_vectorized, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_data():\n",
    "    df = pd.read_csv('clean_scraped_data.csv', index_col=[0])\n",
    "    df = prep.map_other_languages(df)\n",
    "    x = df['lemmatized']\n",
    "    y = df['language']\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    #x_vectorized = cv.fit_transform(x)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 7)\n",
    "    \n",
    "    x_train = cv.fit_transform(x_train)\n",
    "    x_test = cv.transform(x_test)\n",
    "    \n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "           ############       Random Forest       ##############     \n",
    "  ######  Creates N number of trees using random starting values  ######\n",
    "########################################################################\n",
    "\n",
    "def random_forest_model(x, y):\n",
    "    \n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        min_samples_leaf=10,\n",
    "        n_estimators=200,\n",
    "        max_depth=5, \n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        n_jobs=-1,\n",
    "        max_features='auto'\n",
    "    )\n",
    "\n",
    "    rf_classifier.fit(x, y)\n",
    "\n",
    "    y_preds = rf_classifier.predict(x)\n",
    "    \n",
    "    return y_preds\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "    ############       Gradient Boosting Classifier       ##############     \n",
    "######  Creates a random forest where each tree learns from the last  ######\n",
    "############################################################################\n",
    "\n",
    "def gradient_booster_model(x_train, y_train, x_test = 0, y_test = 0, test = False):\n",
    "\n",
    "    gradient_booster = GradientBoostingClassifier(\n",
    "                            learning_rate=0.1,\n",
    "                            max_depth = 5,\n",
    "                            n_estimators=200)\n",
    "    if test == False:\n",
    "    \n",
    "        gradient_booster.fit(x_train, y_train)\n",
    "        y_preds = gradient_booster.predict(x_train)\n",
    "        \n",
    "        return y_preds\n",
    "\n",
    "    if test == True:\n",
    "        gradient_booster.fit(x_train, y_train)\n",
    "        y_preds = gradient_booster.predict(x_test)\n",
    "\n",
    "        return y_preds\n",
    "\n",
    "#################################################################\n",
    "############         XG Boosting Classifier       ##############     \n",
    "    #######       Uses XG Boosting Algorthm       #######\n",
    "#################################################################\n",
    "\n",
    "def xgboost_model(x_train, y_train, x_test = 0, y_test = 0, test = False):\n",
    "    \n",
    "    xgb_params = {'max_depth'       : 3,\n",
    "                  'eta'             : 0.01,\n",
    "                  'silent'          : 0,\n",
    "                  'eval_metric'     : 'auc',\n",
    "                  'subsample'       : 0.8,\n",
    "                  'colsample_bytree': 0.8,\n",
    "                  'objective'       : 'binary:logistic'}\n",
    "\n",
    "    xgboost = xgb(params = xgb_params,\n",
    "                 num_boost_round = 2000,\n",
    "                 verbose_eval = 50,\n",
    "                 early_stopping_rounds = 500,\n",
    "                 #feval = f1_score_cust,\n",
    "                 #evals = evals,\n",
    "                 maximize = True)\n",
    "\n",
    "    if test == False:\n",
    "        xgboost.fit(x_train, y_train)\n",
    "        y_preds = xgboost.predict(x_train)\n",
    "\n",
    "        return y_preds\n",
    "\n",
    "    if test == True:\n",
    "        xgboost.fit(x_train, y_train)\n",
    "        y_preds = xgboost.predict(x_test)\n",
    "\n",
    "    return y_preds\n",
    "    \n",
    "\n",
    "#################################################################\n",
    "#########         LightGMB Boosting Classifier       ###########     \n",
    "#######       Uses Light Gradient Boosting Algorthm       #######\n",
    "#################################################################\n",
    "\n",
    "def lgmboost_model(x, y):\n",
    "    \n",
    "    lgmboost = LGBMClassifier(\n",
    "                learning_rate=0.1,\n",
    "                max_depth = 5,\n",
    "                n_estimators=200)\n",
    "\n",
    "    lgmboost.fit(x, y)\n",
    "    \n",
    "    y_preds = lgmboost.predict(x)\n",
    "    \n",
    "    return y_preds\n",
    "\n",
    "\n",
    "#################################################################\n",
    "#########       HistGradientBoosting Classifier      ###########     \n",
    "#######    Inspired by Light Gradient Boosting Algorthm    ######\n",
    "#################################################################\n",
    "\n",
    "def histgradientboost_model(x_train, y_train, x_test = 0, y_test = 0, test = False):\n",
    "    \n",
    "    HGboost = HistGradientBoostingClassifier(\n",
    "                                            learning_rate=0.1,\n",
    "                                            max_depth = 5)\n",
    "   \n",
    "    HGboost.fit(x_train, y_train)\n",
    "    \n",
    "    if test == False:\n",
    "        y_preds = HGBoost.predict(x_train)\n",
    "        \n",
    "        return y_preds\n",
    "        \n",
    "    if test == True:\n",
    "        y_preds = HGBoost.predict(x_test)\n",
    "    \n",
    "        return y_preds\n",
    "\n",
    "\n",
    "##########################################################\n",
    "#########         Cat Boost Classifier       ###########     \n",
    "#######       Cat Boost Gradient Boosting Algorthm       ##\n",
    "##########################################################\n",
    "\n",
    "def catboost_model(x, y):\n",
    "    \n",
    "    catboost_params = {'loss_function' : 'Logloss',\n",
    "                        'eval_metric' : 'AUC',\n",
    "                        'verbose' : 200}\n",
    "                      \n",
    "    catboost = CatBoostClassifier()\n",
    "\n",
    "    catboost.fit(x, y, use_best_model = True, plot = True)\n",
    "    \n",
    "    y_preds = lgmboost.predict(x)\n",
    "    \n",
    "    return y_preds\n",
    "\n",
    "####################################################################\n",
    "#########         Multinomial Naive Bayes Classifier     ###########     \n",
    "#######     Uses Naive Bayes as Classification Algorithm     #######\n",
    "####################################################################\n",
    "\n",
    "def nb_model(x_train, y_train, x_test = 0, y_test = 0, test = False):\n",
    "    \n",
    "    naive_bayes = MultinomialNB()\n",
    "    \n",
    "    if test == False:\n",
    "        naive_bayes.fit(x_train, y_train)\n",
    "        y_preds = naive_bayes.predict(x_train)\n",
    "\n",
    "        return y_preds\n",
    "    \n",
    "    if test == True:\n",
    "        naive_bayes.fit(x_train, y_train)\n",
    "        y_preds = naive_bayes.predict(x_test)\n",
    "\n",
    "        return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Begins Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 544 ms, sys: 44.6 ms, total: 589 ms\n",
      "Wall time: 594 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, y_train, x_test, y_test = get_split_data()\n",
    "\n",
    "x,y = get_xy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = 'alert alert-block alert-info'>\n",
    "\n",
    "## Testing Bayes Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does classifier perform using train / test split? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Java       0.95      1.00      0.97       276\n",
      "  JavaScript       0.97      0.59      0.73        61\n",
      "       Other       0.89      0.95      0.92       230\n",
      "      Python       0.95      0.82      0.88        51\n",
      "\n",
      "    accuracy                           0.92       618\n",
      "   macro avg       0.94      0.84      0.88       618\n",
      "weighted avg       0.93      0.92      0.92       618\n",
      "\n",
      "CPU times: user 19.2 ms, sys: 2.92 ms, total: 22.2 ms\n",
      "Wall time: 20.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NB_y_preds_train = nb_model(x_train, y_train)\n",
    "report = classification_report(y_train, NB_y_preds_train)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Java       0.80      0.83      0.81       126\n",
      "  JavaScript       0.60      0.12      0.20        25\n",
      "       Other       0.54      0.76      0.63        90\n",
      "      Python       1.00      0.12      0.21        25\n",
      "\n",
      "    accuracy                           0.67       266\n",
      "   macro avg       0.73      0.46      0.46       266\n",
      "weighted avg       0.71      0.67      0.64       266\n",
      "\n",
      "CPU times: user 11.8 ms, sys: 1.73 ms, total: 13.6 ms\n",
      "Wall time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NB_y_preds_test = nb_model(x_train, y_train, x_test, y_test, test=True)\n",
    "report = classification_report(y_test, NB_y_preds_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does classifier perform using kfold cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0    0.646\n",
      "Name: accuracy, dtype: float64 \n",
      "Std:  0.027 \n",
      "Run time: 0    4.033\n",
      "Name: speed, dtype: float64\n",
      "CPU times: user 382 ms, sys: 152 ms, total: 534 ms\n",
      "Wall time: 4.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x,y = get_xy()\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "\n",
    "testing_df = pd.DataFrame(columns = ['speed', 'accuracy'])\n",
    "\n",
    "start = time()\n",
    "cv = RepeatedStratifiedKFold(n_splits = 4, n_repeats = 2, random_state = 7)\n",
    "score = cross_val_score(model, x, y, scoring='accuracy', cv = cv, n_jobs=-1)\n",
    "\n",
    "speed = np.round(time() - start, 3)\n",
    "accuracy = np.mean(score).round(3)\n",
    "\n",
    "testing_df.loc[0] = (speed, accuracy)\n",
    "                          \n",
    "print(f\"Mean Accuracy: {testing_df['accuracy']} \\nStd: {np.std(score): .3f} \\nRun time: {testing_df['speed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = 'alert alert-block alert-info'>\n",
    "    \n",
    "## testing sklearn's gradient booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 739 ms, sys: 68.6 ms, total: 807 ms\n",
      "Wall time: 843 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, y_train, x_test, y_test = get_split_data()\n",
    "x,y = get_xy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Java       1.00      0.99      1.00       276\n",
      "  JavaScript       1.00      1.00      1.00        61\n",
      "       Other       0.99      1.00      1.00       230\n",
      "      Python       1.00      1.00      1.00        51\n",
      "\n",
      "    accuracy                           1.00       618\n",
      "   macro avg       1.00      1.00      1.00       618\n",
      "weighted avg       1.00      1.00      1.00       618\n",
      "\n",
      "CPU times: user 20.7 s, sys: 124 ms, total: 20.8 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_y_preds_train = gradient_booster_model(x_train, y_train)\n",
    "report = classification_report(y_train, gb_y_preds_train)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Java       0.84      0.83      0.83       126\n",
      "  JavaScript       0.78      0.56      0.65        25\n",
      "       Other       0.64      0.72      0.68        90\n",
      "      Python       0.78      0.72      0.75        25\n",
      "\n",
      "    accuracy                           0.76       266\n",
      "   macro avg       0.76      0.71      0.73       266\n",
      "weighted avg       0.76      0.76      0.76       266\n",
      "\n",
      "CPU times: user 20.4 s, sys: 68.5 ms, total: 20.4 s\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_y_preds_test = gradient_booster_model(x_train, y_train, x_test, y_test, test = True)\n",
    "report = classification_report(y_test, gb_y_preds_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = 'alert alert-block alert-info'>\n",
    "    \n",
    "## testing hist gradient booster (sklearn's version of Light GMB classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 529 ms, sys: 61.5 ms, total: 591 ms\n",
      "Wall time: 592 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "x_train, y_train, x_test, y_test = get_split_data()\n",
    "x,y = get_xy()\n",
    "x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hf/46l4fb0x1bj47kh2tdxlqp1h0000gn/T/ipykernel_60445/3589026883.py\u001b[0m in \u001b[0;36mhistgradientboost_model\u001b[0;34m(x_train, y_train, x_test, y_test, test)\u001b[0m\n\u001b[1;32m    113\u001b[0m                                             max_depth = 5)\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mHGboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# time spent predicting X for gradient and hessians update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0macc_prediction_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_DTYPE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         array = _ensure_sparse_format(\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0;34m\"A sparse matrix was passed, but dense \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;34m\"data is required. Use X.toarray() to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hgb_y_preds_train = histgradientboost_model(x_train, y_train)\n",
    "report = classification_report(y_train, hgb_y_preds_train)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hgb_y_preds_test = histgradientboost_model(x_train, y_train, x_test, y_test, test = True)\n",
    "report = classification_report(y_test, hgb_y_preds_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = 'alert alert-block alert-info'>\n",
    "    \n",
    "## testing xg gradient booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_scraped_data.csv', index_col=[0])\n",
    "df = prep.map_other_languages(df)\n",
    "df['language'] = df['language'].map({'Python': 3, 'Other': 2, 'Java' : 0, 'JavaScript' : 1})\n",
    "\n",
    "x = df['lemmatized']\n",
    "y = df['language']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "#x_vectorized = cv.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 7)\n",
    "\n",
    "x_train = cv.fit_transform(x_train)\n",
    "x_test = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time                                     \n",
    "                                     \n",
    "xgb_preds_train = xgboost_model(x_train, y_train)\n",
    "report = classification_report(y_train, xgb_preds_train)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = get_xy()\n",
    "\n",
    "xgb_params = {'max_depth'       : 3,\n",
    "                      'eta'             : 0.01,\n",
    "                      'silent'          : 0,\n",
    "                      'eval_metric'     : 'auc',\n",
    "                      'subsample'       : 0.8,\n",
    "                      'colsample_bytree': 0.8,\n",
    "                      'objective'       : 'binary:logistic'}\n",
    "    \n",
    "    \n",
    "#dtrain = xgb.DMatrix(x_train, y_train, feature_names = x_train.columns.values)\n",
    "#dtest  = xgb.DMatrix(x_test, y_test, feature_names = x_test.columns.values)\n",
    "\n",
    "\n",
    "xgboost = xgb(params = xgb_params,\n",
    "                     num_boost_round = 2000,\n",
    "                     verbose_eval = 50,\n",
    "                     early_stopping_rounds = 500,\n",
    "                     #feval = f1_score_cust,\n",
    "                     #evals = evals,\n",
    "                     maximize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histgradientboost_model(x, y):\n",
    "    \n",
    "    HGboost = HistGradientBoostingClassifier(\n",
    "                                            learning_rate=0.1,\n",
    "                                            max_depth = 5)\n",
    "\n",
    "    HGboost.fit(x, y)\n",
    "    \n",
    "    y_preds = lgmboost.predict(x)\n",
    "    \n",
    "    return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [MultinomialNB(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "For evaluating multiple scores, use sklearn.model_selection.cross_validate instead. ['accuracy', 'f1'] was passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \"\"\"\n\u001b[1;32m    506\u001b[0m     \u001b[0;31m# To ensure multimetric format is not supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     cv_results = cross_validate(\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    480\u001b[0m             )\n\u001b[1;32m    481\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    483\u001b[0m             \u001b[0;34m\"For evaluating multiple scores, use \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0;34m\"sklearn.model_selection.cross_validate instead. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: For evaluating multiple scores, use sklearn.model_selection.cross_validate instead. ['accuracy', 'f1'] was passed."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x,y = get_xy()\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "\n",
    "testing_df = pd.DataFrame(columns = ['speed', 'accuracy'])\n",
    "\n",
    "start = time()\n",
    "cv = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 2, random_state = 7)\n",
    "score = cross_val_score(model, x, y, scoring='accuracy', cv = cv, n_jobs=-1)\n",
    "\n",
    "speed = np.round(time() - start, 3)\n",
    "accuracy = np.mean(score).round(3)\n",
    "\n",
    "testing_df.loc[0] = (speed, accuracy)\n",
    "                          \n",
    "print(f\"Mean Accuracy: {testing_df['accuracy']} \\nStd: {np.std(score): .3f} \\nRun time: {testing_df['speed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6532742681047765"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = pd.DataFrame(columns = ['speed', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.747</td>\n",
       "      <td>0.653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speed  accuracy\n",
       "0  0.747     0.653"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
