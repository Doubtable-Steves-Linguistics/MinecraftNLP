{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minecraft ReadMe Modeling\n",
    "\n",
    "### Table of Contents\n",
    "- [Modeling Section Beginning](#Modeling-Begins-Here)\n",
    "- [Module Functions](#Modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import personal modules\n",
    "import prepare as prep\n",
    "#import acquire as ac\n",
    "\n",
    "\n",
    "#import datascience libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn modules including classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # Gradient Boosting Classifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier # Sklearn version of LGBM Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes Classifier\n",
    "\n",
    "\n",
    "# Sklearn testing, evaluating, and managing model\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# additional, advanced classifiers\n",
    "from xgboost import XGBClassifier as xgb  # XG Boost Classifier\n",
    "from xgboost import DMatrix  # used to transform series to matrix for xg classifier\n",
    "from lightgbm import LGBMClassifier # Light Gradient Boost Classifier\n",
    "from catboost import CatBoostClassifier # Cat boost classifier\n",
    "\n",
    "\n",
    "# import modules from standard library\n",
    "from time import time\n",
    "from pprint import pprint # pretty print\n",
    "from importlib import reload\n",
    "import os\n",
    "\n",
    "\n",
    "# NLP related modules / libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk #Natural Language Tool Kit\n",
    "import re   #Regular Expressions\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'prepare' from '/Users/CryanRedrose/codeup-data-science/CodeUp/MinecraftNLP/prepare.py'>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    \n",
    "    if os.path.isfile('prepared_data.csv'):\n",
    "        return pd.read_csv('prepared_data.csv', index_col=[0])\n",
    "    else:\n",
    "        df = pd.read_csv('clean_scraped_data.csv', index_col=[0])\n",
    "        df = prep.map_other_languages(df)\n",
    "        \n",
    "        df.to_csv('prepared_data.csv', index=False)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy():\n",
    "    df = get_df()\n",
    "        \n",
    "    x = df['lemmatized']\n",
    "    y = df['language']\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    x_vectorized = cv.fit_transform(x)\n",
    "\n",
    "    \n",
    "    \n",
    "    return x_vectorized, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_data():\n",
    "    \n",
    "    df = get_df()\n",
    "    \n",
    "    x = df['lemmatized']\n",
    "    y = df['language']\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    #x_vectorized = cv.fit_transform(x)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 7)\n",
    "    \n",
    "    x_train = cv.fit_transform(x_train)\n",
    "    x_test = cv.transform(x_test)\n",
    "    \n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "           ############       Random Forest       ##############     \n",
    "  ######  Creates N number of trees using random starting values  ######\n",
    "########################################################################\n",
    "\n",
    "def random_forest_model(x, y):\n",
    "    \n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        min_samples_leaf=10,\n",
    "        n_estimators=200,\n",
    "        max_depth=5, \n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        n_jobs=-1,\n",
    "        max_features='auto'\n",
    "    )\n",
    "\n",
    "    rf_classifier.fit(x, y)\n",
    "\n",
    "    y_preds = rf_classifier.predict(x)\n",
    "    \n",
    "    return y_preds\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "    ############       Gradient Boosting Classifier       ##############     \n",
    "######  Creates a random forest where each tree learns from the last  ######\n",
    "############################################################################\n",
    "\n",
    "def gradient_booster_model(x_train, y_train, x_test = 0, y_test = 0, test = False):\n",
    "\n",
    "    gradient_booster = GradientBoostingClassifier(\n",
    "                            learning_rate=0.1,\n",
    "                            max_depth = 5,\n",
    "                            n_estimators=200)\n",
    "    if test == False:\n",
    "    \n",
    "        gradient_booster.fit(x_train, y_train)\n",
    "        y_preds = gradient_booster.predict(x_train)\n",
    "        \n",
    "        return y_preds\n",
    "\n",
    "    if test == True:\n",
    "        gradient_booster.fit(x_train, y_train)\n",
    "        y_preds = gradient_booster.predict(x_test)\n",
    "\n",
    "        return y_preds\n",
    "\n",
    "#################################################################\n",
    "############         XG Boosting Classifier       ##############     \n",
    "    #######       Uses XG Boosting Algorthm       #######\n",
    "#################################################################\n",
    "\n",
    "def xgboost_model(x_train, y_train, x_test = 0, y_test = 0, test = False):\n",
    "\n",
    "    xgb_params = {'max_depth'       : 3,\n",
    "                  'eta'             : 0.01,\n",
    "                  'silent'          : 0,\n",
    "                  'eval_metric'     : 'auc',\n",
    "                  'subsample'       : 0.8,\n",
    "                  'colsample_bytree': 0.8,\n",
    "                  'objective'       : 'binary:logistic'}\n",
    "\n",
    "    \n",
    "    xgboost = xgb(params = xgb_params,\n",
    "                 num_boost_round = 2000,\n",
    "                 verbose_eval = 50,\n",
    "                 #early_stopping_rounds = 500,\n",
    "                 #feval = f1_score_cust,\n",
    "                 #evals = evals,\n",
    "                 maximize = True)\n",
    "    xgboost.fit(x_train, y_train)\n",
    "    \n",
    "    \n",
    "    if test == False:\n",
    "        y_preds = xgboost.predict(x_train)\n",
    "\n",
    "        return y_preds\n",
    "\n",
    "    if test == True:\n",
    "        y_preds = xgboost.predict(x_test)\n",
    "\n",
    "        return y_preds\n",
    "    \n",
    "\n",
    "#################################################################\n",
    "#########         LightGMB Boosting Classifier       ###########     \n",
    "#######       Uses Light Gradient Boosting Algorthm       #######\n",
    "#################################################################\n",
    "\n",
    "def lgmboost_model(x, y):\n",
    "    \n",
    "    lgmboost = LGBMClassifier(\n",
    "                learning_rate=0.1,\n",
    "                max_depth = 5,\n",
    "                n_estimators=200)\n",
    "\n",
    "    lgmboost.fit(x, y)\n",
    "    \n",
    "    y_preds = lgmboost.predict(x)\n",
    "    \n",
    "    return y_preds\n",
    "\n",
    "\n",
    "#################################################################\n",
    "#########       HistGradientBoosting Classifier      ###########     \n",
    "#######    Inspired by Light Gradient Boosting Algorthm    ######\n",
    "#################################################################\n",
    "\n",
    "def histgradientboost_model(x_train, y_train, x_test = 0, y_test = 0, test = False):\n",
    "    \n",
    "    HGboost = HistGradientBoostingClassifier(\n",
    "                                            learning_rate=0.1,\n",
    "                                            max_depth = 5)\n",
    "   \n",
    "    HGboost.fit(x_train, y_train)\n",
    "    \n",
    "    if test == False:\n",
    "        y_preds = HGBoost.predict(x_train)\n",
    "        \n",
    "        return y_preds\n",
    "        \n",
    "    if test == True:\n",
    "        y_preds = HGBoost.predict(x_test)\n",
    "    \n",
    "        return y_preds\n",
    "\n",
    "\n",
    "##########################################################\n",
    "#########         Cat Boost Classifier       ###########     \n",
    "#######       Cat Boost Gradient Boosting Algorthm       ##\n",
    "##########################################################\n",
    "\n",
    "def catboost_model(x, y):\n",
    "    \n",
    "    catboost_params = {'loss_function' : 'Logloss',\n",
    "                        'eval_metric' : 'AUC',\n",
    "                        'verbose' : 200}\n",
    "                      \n",
    "    catboost = CatBoostClassifier()\n",
    "\n",
    "    catboost.fit(x, y, use_best_model = True, plot = True)\n",
    "    \n",
    "    y_preds = lgmboost.predict(x)\n",
    "    \n",
    "    return y_preds\n",
    "\n",
    "####################################################################\n",
    "#########         Multinomial Naive Bayes Classifier     ###########     \n",
    "#######     Uses Naive Bayes as Classification Algorithm     #######\n",
    "####################################################################\n",
    "\n",
    "def nb_model(x_train, y_train, x_test = 0, y_test = 0, test = False):\n",
    "    \n",
    "    naive_bayes = MultinomialNB()\n",
    "    \n",
    "    if test == False:\n",
    "        naive_bayes.fit(x_train, y_train)\n",
    "        y_preds = naive_bayes.predict(x_train)\n",
    "\n",
    "        return y_preds\n",
    "    \n",
    "    if test == True:\n",
    "        naive_bayes.fit(x_train, y_train)\n",
    "        y_preds = naive_bayes.predict(x_test)\n",
    "\n",
    "        return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Begins Here\n",
    "\n",
    "[Back to top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = 'alert alert-block alert-info'>\n",
    "\n",
    "## Testing Bayes Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 517 ms, sys: 44.9 ms, total: 562 ms\n",
      "Wall time: 565 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, y_train, x_test, y_test = get_split_data()\n",
    "\n",
    "x,y = get_xy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does classifier perform using train / test split? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Java       0.95      1.00      0.97       276\n",
      "  JavaScript       0.97      0.59      0.73        61\n",
      "       Other       0.89      0.95      0.92       230\n",
      "      Python       0.95      0.82      0.88        51\n",
      "\n",
      "    accuracy                           0.92       618\n",
      "   macro avg       0.94      0.84      0.88       618\n",
      "weighted avg       0.93      0.92      0.92       618\n",
      "\n",
      "CPU times: user 18.9 ms, sys: 2.44 ms, total: 21.3 ms\n",
      "Wall time: 20 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NB_y_preds_train = nb_model(x_train, y_train)\n",
    "report = classification_report(y_train, NB_y_preds_train)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Java       0.80      0.83      0.81       126\n",
      "  JavaScript       0.60      0.12      0.20        25\n",
      "       Other       0.54      0.76      0.63        90\n",
      "      Python       1.00      0.12      0.21        25\n",
      "\n",
      "    accuracy                           0.67       266\n",
      "   macro avg       0.73      0.46      0.46       266\n",
      "weighted avg       0.71      0.67      0.64       266\n",
      "\n",
      "CPU times: user 12.8 ms, sys: 1.72 ms, total: 14.5 ms\n",
      "Wall time: 12.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NB_y_preds_test = nb_model(x_train, y_train, x_test, y_test, test=True)\n",
    "report = classification_report(y_test, NB_y_preds_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does classifier perform using kfold cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0    0.646\n",
      "Name: accuracy, dtype: float64 \n",
      "Std:  0.027 \n",
      "Run time: 0    3.713\n",
      "Name: speed, dtype: float64\n",
      "CPU times: user 407 ms, sys: 213 ms, total: 621 ms\n",
      "Wall time: 4.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x,y = get_xy()\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "\n",
    "testing_df = pd.DataFrame(columns = ['speed', 'accuracy'])\n",
    "\n",
    "start = time()\n",
    "cv = RepeatedStratifiedKFold(n_splits = 4, n_repeats = 2, random_state = 7)\n",
    "score = cross_val_score(model, x, y, scoring='accuracy', cv = cv, n_jobs=-1)\n",
    "\n",
    "speed = np.round(time() - start, 3)\n",
    "accuracy = np.mean(score).round(3)\n",
    "\n",
    "testing_df.loc[0] = (speed, accuracy)\n",
    "                          \n",
    "print(f\"Mean Accuracy: {testing_df['accuracy']} \\nStd: {np.std(score): .3f} \\nRun time: {testing_df['speed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = 'alert alert-block alert-info'>\n",
    "    \n",
    "## testing sklearn's gradient booster\n",
    "    \n",
    "[Back to top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 514 ms, sys: 38.7 ms, total: 552 ms\n",
      "Wall time: 555 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "original prep work\n",
    "'''\n",
    "x_train, y_train, x_test, y_test = get_split_data()\n",
    "x,y = get_xy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_scraped_data.csv', index_col=[0])\n",
    "#df = prep.map_other_languages(df)\n",
    "#df['language'] = df['language'].map({'Python': 3, 'Other': 2, 'Java' : 0, 'JavaScript' : 1})\n",
    "\n",
    "x = df['lemmatized']\n",
    "y = df['language']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "#x_vectorized = cv.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 7)\n",
    "\n",
    "x_train = cv.fit_transform(x_train)\n",
    "x_test = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Java       1.00      0.99      1.00       276\n",
      "  JavaScript       1.00      1.00      1.00        61\n",
      "       Other       0.99      1.00      1.00       230\n",
      "      Python       1.00      1.00      1.00        51\n",
      "\n",
      "    accuracy                           1.00       618\n",
      "   macro avg       1.00      1.00      1.00       618\n",
      "weighted avg       1.00      1.00      1.00       618\n",
      "\n",
      "CPU times: user 20.2 s, sys: 70.8 ms, total: 20.3 s\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_y_preds_train = gradient_booster_model(x_train, y_train)\n",
    "report = classification_report(y_train, gb_y_preds_train)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Java       0.84      0.82      0.83       126\n",
      "  JavaScript       0.78      0.56      0.65        25\n",
      "       Other       0.63      0.72      0.67        90\n",
      "      Python       0.74      0.68      0.71        25\n",
      "\n",
      "    accuracy                           0.75       266\n",
      "   macro avg       0.75      0.69      0.72       266\n",
      "weighted avg       0.76      0.75      0.75       266\n",
      "\n",
      "CPU times: user 20 s, sys: 46.5 ms, total: 20.1 s\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_y_preds_test = gradient_booster_model(x_train, y_train, x_test, y_test, test = True)\n",
    "report = classification_report(y_test, gb_y_preds_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tried gradient_booster_model without using 'other' option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        Assembly       1.00      1.00      1.00         1\n",
      "       Batchfile       1.00      1.00      1.00         3\n",
      "               C       1.00      1.00      1.00        10\n",
      "              C#       1.00      1.00      1.00        31\n",
      "             C++       1.00      1.00      1.00        29\n",
      "           CMake       1.00      1.00      1.00         1\n",
      "             CSS       1.00      1.00      1.00         3\n",
      "         Clojure       1.00      1.00      1.00         1\n",
      "    CoffeeScript       1.00      1.00      1.00         2\n",
      "      Dockerfile       1.00      1.00      1.00         4\n",
      "          Elixir       1.00      1.00      1.00         1\n",
      "        GDScript       1.00      1.00      1.00         1\n",
      "            GLSL       1.00      1.00      1.00         4\n",
      "              Go       1.00      1.00      1.00        16\n",
      "             HCL       1.00      1.00      1.00         3\n",
      "            HTML       1.00      1.00      1.00         3\n",
      "         Haskell       1.00      1.00      1.00         1\n",
      "            JSON       1.00      1.00      1.00         1\n",
      "            Java       1.00      1.00      1.00       276\n",
      "      JavaScript       1.00      1.00      1.00        61\n",
      "Jupyter Notebook       1.00      1.00      1.00         2\n",
      "          Kotlin       1.00      1.00      1.00        16\n",
      "             Lua       1.00      1.00      1.00         5\n",
      "        Mustache       1.00      1.00      1.00         1\n",
      "             PHP       1.00      1.00      1.00        23\n",
      "      PowerShell       1.00      1.00      1.00         1\n",
      "       PureBasic       1.00      1.00      1.00         1\n",
      "          Python       1.00      1.00      1.00        51\n",
      "               R       1.00      1.00      1.00         1\n",
      "            Ruby       1.00      1.00      1.00         4\n",
      "            Rust       1.00      1.00      1.00         9\n",
      "            SCSS       1.00      1.00      1.00         1\n",
      "           Scala       1.00      1.00      1.00         1\n",
      "           Shell       1.00      1.00      1.00        27\n",
      "          Smarty       1.00      1.00      1.00         1\n",
      "   SystemVerilog       1.00      1.00      1.00         1\n",
      "             TeX       1.00      1.00      1.00         2\n",
      "      TypeScript       1.00      1.00      1.00        14\n",
      "            Yacc       1.00      1.00      1.00         1\n",
      "       ZenScript       1.00      1.00      1.00         4\n",
      "\n",
      "        accuracy                           1.00       618\n",
      "       macro avg       1.00      1.00      1.00       618\n",
      "    weighted avg       1.00      1.00      1.00       618\n",
      "\n",
      "CPU times: user 2min 47s, sys: 583 ms, total: 2min 48s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_y_preds_train = gradient_booster_model(x_train, y_train)\n",
    "report = classification_report(y_train, gb_y_preds_train)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       AutoHotkey       0.00      0.00      0.00         1\n",
      "                C       0.50      0.17      0.25         6\n",
      "               C#       0.60      0.50      0.55        12\n",
      "              C++       0.75      0.43      0.55         7\n",
      "              CSS       0.00      0.00      0.00         1\n",
      "     CoffeeScript       0.00      0.00      0.00         1\n",
      "           Elixir       0.00      0.00      0.00         1\n",
      "               Go       1.00      0.33      0.50         3\n",
      "             HTML       0.00      0.00      0.00         3\n",
      "             Java       0.68      0.94      0.79       126\n",
      "       JavaScript       0.67      0.56      0.61        25\n",
      " Jupyter Notebook       0.00      0.00      0.00         2\n",
      "           Kotlin       1.00      0.14      0.25         7\n",
      "              Lua       0.00      0.00      0.00         1\n",
      "         Mustache       0.00      0.00      0.00         1\n",
      "              PHP       0.60      0.60      0.60        10\n",
      "       PowerShell       0.00      0.00      0.00         1\n",
      "           Python       0.81      0.84      0.82        25\n",
      "             Ruby       0.00      0.00      0.00         1\n",
      "             Rust       1.00      0.67      0.80         6\n",
      "            Scala       0.00      0.00      0.00         3\n",
      "            Shell       0.33      0.12      0.18         8\n",
      "            Swift       0.00      0.00      0.00         1\n",
      "       TypeScript       0.29      0.25      0.27         8\n",
      "Visual Basic .NET       0.00      0.00      0.00         2\n",
      "              Vue       0.00      0.00      0.00         1\n",
      "        ZenScript       0.00      0.00      0.00         1\n",
      "       mcfunction       0.00      0.00      0.00         2\n",
      "\n",
      "         accuracy                           0.67       266\n",
      "        macro avg       0.29      0.20      0.22       266\n",
      "     weighted avg       0.62      0.67      0.62       266\n",
      "\n",
      "CPU times: user 2min 43s, sys: 689 ms, total: 2min 44s\n",
      "Wall time: 2min 44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_y_preds_test = gradient_booster_model(x_train, y_train, x_test, y_test, test = True)\n",
    "report = classification_report(y_test, gb_y_preds_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying new data frame's cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "''' \n",
    "after new prep changes were created\n",
    "'''\n",
    "x_train, y_train, x_test, y_test = get_split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Java       1.00      0.99      1.00       276\n",
      "  JavaScript       1.00      1.00      1.00        61\n",
      "       Other       0.99      1.00      1.00       230\n",
      "      Python       1.00      1.00      1.00        51\n",
      "\n",
      "    accuracy                           1.00       618\n",
      "   macro avg       1.00      1.00      1.00       618\n",
      "weighted avg       1.00      1.00      1.00       618\n",
      "\n",
      "CPU times: user 21 s, sys: 129 ms, total: 21.1 s\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_y_preds_train = gradient_booster_model(x_train, y_train)\n",
    "report = classification_report(y_train, gb_y_preds_train)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Java       0.84      0.82      0.83       126\n",
      "  JavaScript       0.79      0.60      0.68        25\n",
      "       Other       0.65      0.73      0.69        90\n",
      "      Python       0.78      0.72      0.75        25\n",
      "\n",
      "    accuracy                           0.76       266\n",
      "   macro avg       0.77      0.72      0.74       266\n",
      "weighted avg       0.77      0.76      0.76       266\n",
      "\n",
      "CPU times: user 20.8 s, sys: 107 ms, total: 20.9 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_y_preds_test = gradient_booster_model(x_train, y_train, x_test, y_test, test = True)\n",
    "report = classification_report(y_test, gb_y_preds_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = 'alert alert-block alert-info'>\n",
    "    \n",
    "## testing hist gradient booster (sklearn's version of Light GMB classifier)\n",
    "    \n",
    "[Back to top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 534 ms, sys: 61.9 ms, total: 596 ms\n",
      "Wall time: 599 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, y_train, x_test, y_test = get_split_data()\n",
    "x,y = get_xy()\n",
    "x_train = x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HGBoost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hf/46l4fb0x1bj47kh2tdxlqp1h0000gn/T/ipykernel_60445/3589026883.py\u001b[0m in \u001b[0;36mhistgradientboost_model\u001b[0;34m(x_train, y_train, x_test, y_test, test)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHGBoost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HGBoost' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hgb_y_preds_train = histgradientboost_model(x_train, y_train)\n",
    "report = classification_report(y_train, hgb_y_preds_train)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hgb_y_preds_test = histgradientboost_model(x_train, y_train, x_test, y_test, test = True)\n",
    "report = classification_report(y_test, hgb_y_preds_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = 'alert alert-block alert-info'>\n",
    "    \n",
    "## testing xg gradient booster\n",
    "    \n",
    "[Back to top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df()\n",
    "df['language'] = df['language'].map({'Python': 3, 'Other': 2, 'Java' : 0, 'JavaScript' : 1})\n",
    "\n",
    "x = df['lemmatized']\n",
    "y = df['language']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "#x_vectorized = cv.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 7)\n",
    "\n",
    "x_train = cv.fit_transform(x_train)\n",
    "x_test = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:31:22] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"maximize\", \"num_boost_round\", \"params\", \"verbose_eval\" } are not used.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       276\n",
      "           1       1.00      0.97      0.98        61\n",
      "           2       1.00      1.00      1.00       230\n",
      "           3       1.00      1.00      1.00        51\n",
      "\n",
      "    accuracy                           1.00       618\n",
      "   macro avg       1.00      0.99      0.99       618\n",
      "weighted avg       1.00      1.00      1.00       618\n",
      "\n",
      "CPU times: user 15.1 s, sys: 181 ms, total: 15.3 s\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time                                     \n",
    "                                     \n",
    "xgb_preds_train = xgboost_model(x_train, y_train)\n",
    "report = classification_report(y_train, xgb_preds_train)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:32:06] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"maximize\", \"num_boost_round\", \"params\", \"verbose_eval\" } are not used.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       126\n",
      "           1       0.75      0.60      0.67        25\n",
      "           2       0.65      0.68      0.66        90\n",
      "           3       0.83      0.76      0.79        25\n",
      "\n",
      "    accuracy                           0.75       266\n",
      "   macro avg       0.76      0.72      0.74       266\n",
      "weighted avg       0.75      0.75      0.75       266\n",
      "\n",
      "CPU times: user 15 s, sys: 171 ms, total: 15.1 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time                                     \n",
    "                                     \n",
    "xgb_preds_test = xgboost_model(x_train, y_train, x_test, y_test, test = True)\n",
    "report = classification_report(y_test, xgb_preds_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = get_xy()\n",
    "\n",
    "xgb_params = {'max_depth'       : 3,\n",
    "                      'eta'             : 0.01,\n",
    "                      'silent'          : 0,\n",
    "                      'eval_metric'     : 'auc',\n",
    "                      'subsample'       : 0.8,\n",
    "                      'colsample_bytree': 0.8,\n",
    "                      'objective'       : 'binary:logistic'}\n",
    "    \n",
    "    \n",
    "#dtrain = xgb.DMatrix(x_train, y_train, feature_names = x_train.columns.values)\n",
    "#dtest  = xgb.DMatrix(x_test, y_test, feature_names = x_test.columns.values)\n",
    "\n",
    "\n",
    "xgboost = xgb(params = xgb_params,\n",
    "                     num_boost_round = 2000,\n",
    "                     verbose_eval = 50,\n",
    "                     early_stopping_rounds = 500,\n",
    "                     #feval = f1_score_cust,\n",
    "                     #evals = evals,\n",
    "                     maximize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histgradientboost_model(x, y):\n",
    "    \n",
    "    HGboost = HistGradientBoostingClassifier(\n",
    "                                            learning_rate=0.1,\n",
    "                                            max_depth = 5)\n",
    "\n",
    "    HGboost.fit(x, y)\n",
    "    \n",
    "    y_preds = lgmboost.predict(x)\n",
    "    \n",
    "    return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [MultinomialNB(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "For evaluating multiple scores, use sklearn.model_selection.cross_validate instead. ['accuracy', 'f1'] was passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \"\"\"\n\u001b[1;32m    506\u001b[0m     \u001b[0;31m# To ensure multimetric format is not supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     cv_results = cross_validate(\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    480\u001b[0m             )\n\u001b[1;32m    481\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    483\u001b[0m             \u001b[0;34m\"For evaluating multiple scores, use \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0;34m\"sklearn.model_selection.cross_validate instead. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: For evaluating multiple scores, use sklearn.model_selection.cross_validate instead. ['accuracy', 'f1'] was passed."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x,y = get_xy()\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "\n",
    "testing_df = pd.DataFrame(columns = ['speed', 'accuracy'])\n",
    "\n",
    "start = time()\n",
    "cv = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 2, random_state = 7)\n",
    "score = cross_val_score(model, x, y, scoring='accuracy', cv = cv, n_jobs=-1)\n",
    "\n",
    "speed = np.round(time() - start, 3)\n",
    "accuracy = np.mean(score).round(3)\n",
    "\n",
    "testing_df.loc[0] = (speed, accuracy)\n",
    "                          \n",
    "print(f\"Mean Accuracy: {testing_df['accuracy']} \\nStd: {np.std(score): .3f} \\nRun time: {testing_df['speed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6532742681047765"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = pd.DataFrame(columns = ['speed', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.747</td>\n",
       "      <td>0.653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speed  accuracy\n",
       "0  0.747     0.653"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
