{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a7a023",
   "metadata": {},
   "source": [
    "# *Minecraft NLP Project*\n",
    "### Presented by: Rae Downen, Cristina Lucin, Michael Mesa and John \"Chris\" Rosenberger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a47c59",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e0721",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "This project focuses on building a prediction model for accurately predicting the coding language of a project using examination of GitHub repo Readme files. Our goal is to develop a predictive model utilizing Python and Python libraries and select the most effective model for production. Initially, we are utilizing BeautifulSoup to acquire our data, selecting 1000 repositories tagged with 'Minecraft' from GitHub, taking in all Readme text and repo language information from each repo. After gathering the data, we explore the data through questions and visualizations before developing a model that can tell us: \"What language is this repository most likely to be written in?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e91889",
   "metadata": {},
   "source": [
    "## Goals\n",
    "### Create deliverables:\n",
    "* READ ME\n",
    "* Final Report\n",
    "* Functional acquire.py, explore.py, and model.py files\n",
    "* Acquire data from GitHub utilizing BeautifulSoup to scrape targeted Repositories ('Repos')\n",
    "* Prepare and split the data\n",
    "* Explore the data\n",
    "* Establish a baseline\n",
    "* Fit and train a classification model to predict the programming language of the Repo\n",
    "* Evaluate the model by comparing its performance on train utilizing accuracy as a measure\n",
    "* Evaluate the selected model on test data\n",
    "* Develop and document findings, takeaways, recommendations, and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f339d13",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mexplore\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mprepare\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mp\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodeling\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mm\u001b[39;00m\n",
      "File \u001b[0;32m~/codeup-data-science/MinecraftNLP/modeling.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# additional, advanced classifiers\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier \u001b[38;5;28;01mas\u001b[39;00m xgb  \u001b[38;5;66;03m# XG Boost Classifier\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#from lightgbm import LGBMClassifier # Light Gradient Boost Classifier\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier \u001b[38;5;66;03m# Cat boost classifier\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "#General DS Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "#Modeling, NLP and Exploration\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#My imports\n",
    "import os\n",
    "from env import github_token, github_username\n",
    "from importlib import reload\n",
    "import acquire as a\n",
    "import explore as e\n",
    "import prepare as p\n",
    "import modeling as m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106bca6c",
   "metadata": {},
   "source": [
    "# Acquire\n",
    "* 1,000 Repo URLs tagged \"Minecraft\" were acquired from GitHub utilizing a .py script \"acquire_minecraft_urls.py\"\n",
    "* These Repos were identified and scraped through the search feature in GitHub\n",
    "* Repo Readme Text and Repo Language was scraped utilizing BeautifulSoup\n",
    "* Readme Text and Repo Language was collected into a dictionary using a function called \"process_repo.py\" and \"scrape_github_data\"\n",
    "* This dictionary was turned into a data frame and CSV file\n",
    "* The CSV file contained 1,000 rows and 3 features before cleaning\n",
    "* Each row represents a unique Repo located on GitHub\n",
    "* Each column represents a feature of the Repo, such as URL, Readme text, or Programming Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b457f780",
   "metadata": {},
   "source": [
    "# Prepare\n",
    "#### Prepare Actions:\n",
    "* Renamed columns to improve readability\n",
    "* Removed white space from values in object columns\n",
    "* Checked for null values in the data, dropped all rows where nulls existed\n",
    "* Utilized Regex and string methods and functions to clean Repo Readme text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import our data from a .csv file, take a peek at the data\n",
    "df = pd.read_csv(r'clean_scraped_data.csv', index_col=[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 20 languages from Readme files\n",
    "df.language.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11886e08",
   "metadata": {},
   "source": [
    "### We chose to focus on the top 3 programming languages found in the scraped Repos, classifying all other languages as \"Other\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00361ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Recast other languages as \"Other\"\n",
    "df = p.map_other_languages(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1444e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3dda38d",
   "metadata": {},
   "source": [
    "### Cleaning: We elected to remove reserved words that were in common with all 3 top languages and some that were both in common between Java and JavaScript.  We also removed words such as 'minecraft', 'server', 'run', etc. by utilizing stop words through a prepare function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords from dataframe\n",
    "df = p.prep_readme_data(df, 'readme_contents')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47e505",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bbcb17",
   "metadata": {},
   "source": [
    "* For exploration, we chose to do a train test split taking 20% for test, %30 of that for validate, and the remainder for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = e.split_minecraft_data(df)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94ea1f",
   "metadata": {},
   "source": [
    "## Question 1: What are the top programming languages found in #Minecraft GitHub Repos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.get_language_freq(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd2287",
   "metadata": {},
   "source": [
    "### Java was the most common language found in the Repos that we scraped from, followed by JavaScript and Python. All other languages are included in this visualization. This information made sense, considering that Minecraft was developed using Java."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff2ba2",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbf138",
   "metadata": {},
   "source": [
    "## Question 2: What is the average wordcount of a Repo Readme file based on their programming language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90895ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.get_wordcount_bar(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adffbbbe",
   "metadata": {},
   "source": [
    "## Question 3: What are the top 10 most frequent words found in Python Repos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa90e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.get_top10_python(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.get_python_wordcloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646c5f4",
   "metadata": {},
   "source": [
    "## Question 4: What are the top 10 most frequent words found in Java Repos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.get_top10_java(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39af0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.get_java_wordcloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef62c5",
   "metadata": {},
   "source": [
    "## Question 5: What are the top 10 most frequent words found in JavaScript Repos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e4c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.get_top10_js(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.get_js_wordcloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c81329",
   "metadata": {},
   "source": [
    "## Exploration Summary\n",
    "* Java was the most frequent language found in the Repositories examined\n",
    "* JavaScript Repos had the highest average wordcount, Java Repos had the lowest\n",
    "* \"Install\" was the most common word for Python Repos\n",
    "* \"Mod\" and \"Build\" were the most frequently found Java strings\n",
    "* \"Command\" was the most frequent word found in JavaScript Repos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82066bd5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6159769",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b7ee09",
   "metadata": {},
   "source": [
    "* We elected to utilize accuracy as the evaluation metric\n",
    "* We developed three different models using different model types: (Naive Bayes, SKLearn Gradient Booster, XG Boost)\n",
    "* The model that performs the best was evaluated on test data\n",
    "* **We utilized the mode of 'language' as the baseline (Java, 45.3)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00712374",
   "metadata": {},
   "source": [
    "We explored several methods of NLP modeling. We elected to utilize as much useful text as possible. This is a multilabel classification project which makes the confusion matrix more complicated than more common classification problems. Due to that, we decided that the large string of text would be more useful for finding differences between the languages used in the Readme files.\n",
    "\n",
    "We trained several models on our training set without hyperparameter tuning to produce models that were 'Good Enough'. These models overfit on training data. A problem with the differentiation within Readme files is that they all utilize normal, human language to describe a programming process. Because the Readme does not necessarily use specialized programming language, this made classification much more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d26db",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f6ee3",
   "metadata": {},
   "source": [
    "## Initial model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b22ec5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f9b3d",
   "metadata": {},
   "source": [
    "### Naive Bayes, SKLearn Gradient Booster, and Extreme Gradient Boosting (XG Boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.get_model_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5ec87",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aacb412",
   "metadata": {},
   "source": [
    "*All models selected overfit the training data. We elected to utilize **SK Learn Gradient Boosting** because SK Learn is an open source algorithm with a lot of support.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31b82f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9ef2b2",
   "metadata": {},
   "source": [
    "### Testing Selected model on unseen (test) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a599b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SK Learn Gradient Boosting Test\n",
    "m.gb_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd04c7",
   "metadata": {},
   "source": [
    "## Modeling Summary\n",
    "* All models were overfit on the training data\n",
    "* SKLearn Gradient Boost was chosen for test data\n",
    "* **This model performed with a 76 percent accuracy, a 30 percent improvement from the baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f608fa55",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83108a04",
   "metadata": {},
   "source": [
    "# Takeaways/ Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055ae6d",
   "metadata": {},
   "source": [
    "- GitHub Repos with different programming languages have significantly different features (word count and unique words)\n",
    "- Because ReadMe files are written in normal language, the accuracy of any model is limited\n",
    "- Improved cleaning methods may increase model performance\n",
    "- Count Vectorization (CV) in combination with ensemble classification is an effective modeling strategy for NLP/Text Classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e562e92",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a5cfe",
   "metadata": {},
   "source": [
    "- Acquire longer Readme text files to feed into algorithm\n",
    "- Narrow down parameters for classifications (more languages are more difficult to classify)\n",
    "* Additional hyperparameter tuning may result in better model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70412376",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0b9ef",
   "metadata": {},
   "source": [
    "* Utilize statistical methods to identify additional stop words\n",
    "* Develop and test different model types for performance\n",
    "* Find alternative methods for pulling repo data from GitHub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
